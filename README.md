# A-MEM: Agentic Memory System

Ein agentisches Memory-System f√ºr LLM Agents basierend auf dem Zettelkasten-Prinzip.

> **Based on:** ["A-Mem: Agentic Memory for LLM Agents"](https://arxiv.org/html/2502.12110v11)  
> by Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang  
> Rutgers University, Independent Researcher, AIOS Foundation

## üöÄ Features

- ‚úÖ **Note Construction**: Automatische Extraktion von Keywords, Tags und Contextual Summary
- ‚úÖ **Link Generation**: Automatische Verkn√ºpfung √§hnlicher Memories
- ‚úÖ **Memory Evolution**: Dynamische Aktualisierung bestehender Memories
- ‚úÖ **Semantic Retrieval**: Intelligente Suche mit Graph-Traversal
- ‚úÖ **Multi-Provider Support**: Ollama (lokal) oder OpenRouter (Cloud)
- ‚úÖ **Environment Variables**: Konfiguration √ºber `.env` Datei

## üìã Installation

### 1. Dependencies installieren

```bash
pip install -r requirements.txt
```

### 2. Environment Variables konfigurieren

Kopiere `.env.example` zu `.env` und passe die Werte an:

```bash
cp .env.example .env
```

**Konfiguration:**

- **LLM_PROVIDER**: `"ollama"` (lokal) oder `"openrouter"` (Cloud)
- **Ollama**: Lokale Modelle (Standard)
- **OpenRouter**: Cloud-basierte LLMs (ben√∂tigt API Key)

**Beispiel `.env` f√ºr Ollama (Standard):**
```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=qwen3:4b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest
```

**Beispiel `.env` f√ºr OpenRouter:**
```env
LLM_PROVIDER=openrouter
OPENROUTER_API_KEY=your_api_key_here
OPENROUTER_LLM_MODEL=openai/gpt-4o-mini
OPENROUTER_EMBEDDING_MODEL=openai/text-embedding-3-small
```

### 3. Ollama Modelle installieren (nur bei LLM_PROVIDER=ollama)

```bash
ollama pull qwen3:4b
ollama pull nomic-embed-text:latest
```

### 4. Ollama starten (nur bei LLM_PROVIDER=ollama)

Stelle sicher, dass Ollama auf `http://localhost:11434` l√§uft.

## üõ†Ô∏è MCP Server

### Start

```bash
python mcp_server.py
```

### Verf√ºgbare Tools

1. **`create_atomic_note`** - Speichert eine neue Information im Memory System
2. **`retrieve_memories`** - Sucht nach relevanten Memories basierend auf semantischer √Ñhnlichkeit
3. **`get_memory_stats`** - Gibt Statistiken √ºber das Memory System zur√ºck
4. **`delete_atomic_note`** - L√∂scht eine Note aus dem Memory System
5. **`add_file`** - Speichert den Inhalt einer Datei (z.B. .md) als Note, unterst√ºtzt automatisches Chunking
6. **`reset_memory`** - Setzt das komplette Memory System zur√ºck (‚ö†Ô∏è nicht r√ºckg√§ngig machbar)

### IDE Integration

#### Cursor IDE

1. √ñffne die MCP-Konfigurationsdatei:
   - Windows: `%APPDATA%\Cursor\User\globalStorage\saoudrizwan.claude-dev\settings\cline_mcp_settings.json`
   - macOS: `~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`
   - Linux: `~/.config/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`

2. F√ºge folgende Konfiguration hinzu:

```json
{
  "mcpServers": {
    "a-mem": {
      "command": "python",
      "args": [
        "-m",
        "src.a_mem.main"
      ],
      "cwd": "/path/to/a-mem-agentic-memory-system"
    }
  }
}
```

**Wichtig:** Passe `cwd` auf den absoluten Pfad zu deinem Projekt-Verzeichnis an!

3. Starte Cursor neu, damit die Konfiguration geladen wird.

#### Visual Studio Code (mit MCP Extension)

1. Installiere die MCP Extension f√ºr VSCode (falls verf√ºgbar)

2. √ñffne die VSCode Settings (JSON):
   - `Ctrl+Shift+P` (Windows/Linux) oder `Cmd+Shift+P` (macOS)
   - Tippe "Preferences: Open User Settings (JSON)"

3. F√ºge die MCP Server Konfiguration hinzu:

```json
{
  "mcp.servers": {
    "a-mem": {
      "command": "python",
      "args": ["-m", "src.a_mem.main"],
      "cwd": "/path/to/a-mem-agentic-memory-system"
    }
  }
}
```

**Alternative:** Nutze die `mcp.json` Datei im Projekt-Root:

```json
{
  "mcpServers": {
    "a-mem": {
      "command": "python",
      "args": ["-m", "src.a_mem.main"],
      "cwd": "${workspaceFolder}"
    }
  }
}
```

#### Verwendung in der IDE

Nach der Konfiguration stehen dir die MCP Tools direkt in der IDE zur Verf√ºgung:

- **Chat/Composer**: Nutze die Tools √ºber nat√ºrliche Sprache
  - "Speichere diese Information: ..."
  - "Suche nach Memories √ºber: ..."
  - "Zeige mir die Memory-Statistiken"

- **Code**: Die Tools werden automatisch als Funktionen verf√ºgbar

Siehe `MCP_SERVER_SETUP.md` f√ºr detaillierte Informationen zu allen verf√ºgbaren Tools.

## üìö Dokumentation

- `docs/ARCHITECTURE.md` - System-Architektur
- `docs/FINAL_COMPLIANCE_CHECK.md` - Paper-Compliance
- `docs/TEST_REPORT.md` - Test-Ergebnisse
- `MCP_SERVER_SETUP.md` - MCP Server Setup

## üß™ Tests

```bash
python tests/test_a_mem.py
python tests/test_code_structure.py
```

## üß™ Benchmarking

Das Projekt enth√§lt ein modernes TUI-Benchmark-Tool f√ºr Ollama-Modelle:

```bash
python ollama_benchmark.py
```

Siehe `BENCHMARK_README.md` f√ºr Details.

## üìä Status

‚úÖ **100% Paper-Compliance**  
‚úÖ **Alle Tests bestanden**  
‚úÖ **Modulare Struktur**  
‚úÖ **Multi-Provider Support** (Ollama + OpenRouter)  
‚úÖ **MCP Server Integration**  
‚úÖ **Memory Reset & Management Tools**

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

This implementation is based on the research paper ["A-Mem: Agentic Memory for LLM Agents"](https://arxiv.org/html/2502.12110v11).

## üôè Acknowledgments

- Original paper authors: Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang
- Original repositories:
  - [AgenticMemory](https://github.com/WujiangXu/AgenticMemory) - Benchmark Evaluation
  - [A-mem-sys](https://github.com/WujiangXu/A-mem-sys) - Production-ready System

---

**Created by tobi and the CURSOR IDE with the new Composer 1 model for the community ‚ù§Ô∏è**
